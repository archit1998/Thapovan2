{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'values1.csv' does not exist: b'values1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-51e6cb556b53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mnew_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"using\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"show\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"result\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"large\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"also\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"one\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"two\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"new\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"previously\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"shown\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mdata1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"values1.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror_bad_lines\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'values1.csv' does not exist: b'values1.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lem = WordNetLemmatizer()\n",
    "stem = PorterStemmer()\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "new_words = [\"using\", \"show\", \"result\", \"large\", \"also\", \"iv\", \"one\", \"two\", \"new\", \"previously\", \"shown\"]\n",
    "stop_words = stop_words.union(new_words)\n",
    "data1 = pd.read_csv(\"values1.csv\", error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\admin\\\\thapovan\\\\Thapovan2\\\\Corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Thapovan_data.csv\", error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Definition</th>\n",
       "      <th>word_count</th>\n",
       "      <th>corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1%/10 Net 30</td>\n",
       "      <td>The 1%/10 net 30 calculation is a way of provi...</td>\n",
       "      <td>39</td>\n",
       "      <td>net calculation way providing cash discount pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100% Equities Strategy</td>\n",
       "      <td>A 100% equities strategy is an investment stra...</td>\n",
       "      <td>39</td>\n",
       "      <td>equity strategy investment strategy individual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100% Mortgage</td>\n",
       "      <td>100% mortgage is a mortgage loan in which the ...</td>\n",
       "      <td>41</td>\n",
       "      <td>mortgage mortgage loan borrower receives loan ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1040 Form</td>\n",
       "      <td>Form 1040 is the standard Internal Revenue Ser...</td>\n",
       "      <td>52</td>\n",
       "      <td>form standard internal revenue service irs for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1040A Form</td>\n",
       "      <td>The 1040A Form is a simplified version of the ...</td>\n",
       "      <td>46</td>\n",
       "      <td>form simplified version form individual income...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Term                                         Definition  \\\n",
       "0            1%/10 Net 30   The 1%/10 net 30 calculation is a way of provi...   \n",
       "1  100% Equities Strategy   A 100% equities strategy is an investment stra...   \n",
       "2           100% Mortgage   100% mortgage is a mortgage loan in which the ...   \n",
       "3               1040 Form   Form 1040 is the standard Internal Revenue Ser...   \n",
       "4              1040A Form   The 1040A Form is a simplified version of the ...   \n",
       "\n",
       "   word_count                                             corpus  \n",
       "0          39  net calculation way providing cash discount pu...  \n",
       "1          39  equity strategy investment strategy individual...  \n",
       "2          41  mortgage mortgage loan borrower receives loan ...  \n",
       "3          52  form standard internal revenue service irs for...  \n",
       "4          46  form simplified version form individual income...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading https://files.pythonhosted.org/packages/81/80/858ef502e80baa6384b75fd5c89f01074b791a13b830487f9e25bdce50ec/gensim-3.7.3.tar.gz (23.4MB)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from gensim) (1.16.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from gensim) (1.2.1)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from gensim) (1.12.0)\n",
      "Collecting smart_open>=1.7.0 (from gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/37/c0/25d19badc495428dec6a4bf7782de617ee0246a9211af75b302a2681dea7/smart_open-1.8.4.tar.gz (63kB)\n",
      "Requirement already satisfied: boto>=2.32 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from smart_open>=1.7.0->gensim) (2.49.0)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (from smart_open>=1.7.0->gensim) (2.21.0)\n",
      "Collecting boto3 (from smart_open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/1b/1b/a0c60c2c8f3f927254a6d76559a30679eb86570df588701cc00f6ba7a468/boto3-1.9.166-py2.py3-none-any.whl (128kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->smart_open>=1.7.0->gensim) (2019.3.9)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->smart_open>=1.7.0->gensim) (1.24.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->smart_open>=1.7.0->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->smart_open>=1.7.0->gensim) (2.8)\n",
      "Collecting botocore<1.13.0,>=1.12.166 (from boto3->smart_open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/de/3e/22df1685637c4ede1288d86417e6ad86e25ae6face70d185075bbf0590db/botocore-1.12.166-py2.py3-none-any.whl (5.5MB)\n",
      "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->smart_open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.3.0,>=0.2.0 (from boto3->smart_open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/16/8a/1fc3dba0c4923c2a76e1ff0d52b305c44606da63f718d14d3231e21c51b0/s3transfer-0.2.1-py2.py3-none-any.whl (70kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in c:\\users\\admin\\anaconda3\\lib\\site-packages (from botocore<1.13.0,>=1.12.166->boto3->smart_open>=1.7.0->gensim) (2.8.0)\n",
      "Requirement already satisfied: docutils>=0.10 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from botocore<1.13.0,>=1.12.166->boto3->smart_open>=1.7.0->gensim) (0.14)\n",
      "Building wheels for collected packages: gensim, smart-open\n",
      "  Building wheel for gensim (setup.py): started\n",
      "  Building wheel for gensim (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\admin\\AppData\\Local\\pip\\Cache\\wheels\\73\\6b\\89\\bb14fd56b74774a39a771a12f525a6a14c2c2692d3084ad048\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\admin\\AppData\\Local\\pip\\Cache\\wheels\\5f\\ea\\fb\\5b1a947b369724063b2617011f1540c44eb00e28c3d2ca8692\n",
      "Successfully built gensim smart-open\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto3-1.9.166 botocore-1.12.166 gensim-3.7.3 jmespath-0.9.4 s3transfer-0.2.1 smart-open-1.8.4\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_match_list(db,trial):\n",
    "    p_list = []\n",
    "    for i in range(0,len(db)):\n",
    "        metric = db['metric_name'][i]\n",
    "        text = re.sub('[^a-zA-Z]', ' ', metric)\n",
    "    \n",
    "        #Convert to lowercase\n",
    "        text = text.lower()\n",
    "    \n",
    "        #remove tags\n",
    "        text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "    \n",
    "        # remove special characters and digits\n",
    "        text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "        ##Convert to list from string\n",
    "        text = text.split()\n",
    "    \n",
    "        ##Stemming\n",
    "        ps=PorterStemmer()\n",
    "        #Lemmatisation\n",
    "        lem = WordNetLemmatizer()\n",
    "        text = [lem.lemmatize(word) for word in text if not word in  \n",
    "            stop_words] \n",
    "        text = \" \".join(text)\n",
    "        tex_check = text.split(\" \")\n",
    "        list=[]\n",
    "        for key in data1[\"Keyword\"]:\n",
    "            count = 0\n",
    "            for j in tex_check:\n",
    "                if(j in key):\n",
    "                    count = count+1\n",
    "            list.append((count/len(tex_check))*100)  \n",
    "        l = []\n",
    "        count1 = 0 \n",
    "        maxi = max(list)\n",
    "        for val in list:\n",
    "            if(val==maxi):\n",
    "                l.append(count1)\n",
    "            count1 = count1+1 \n",
    "        l2=[]\n",
    "        for j in l:\n",
    "            l2.append(data['Term'][j])\n",
    "        p_list.append(l2)      \n",
    "    db['possible'] = p_list\n",
    "    t_list = []\n",
    "    for i in range(0,len(trial)):\n",
    "        metric = trial['trial'][i]\n",
    "        text = re.sub('[^a-zA-Z]', ' ', metric)\n",
    "    \n",
    "        #Convert to lowercase\n",
    "        text = text.lower()\n",
    "    \n",
    "        #remove tags\n",
    "        text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "    \n",
    "        # remove special characters and digits\n",
    "        text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "        ##Convert to list from string\n",
    "        text = text.split()\n",
    "    \n",
    "        ##Stemming\n",
    "        ps=PorterStemmer()\n",
    "        #Lemmatisation\n",
    "        lem = WordNetLemmatizer()\n",
    "        text = [lem.lemmatize(word) for word in text if not word in  \n",
    "            stop_words] \n",
    "        text = \" \".join(text)\n",
    "        tex_check = text.split(\" \")\n",
    "        list=[]\n",
    "        for key in data1[\"Keyword\"]:\n",
    "            count = 0\n",
    "            for j in tex_check:\n",
    "                if(j in key):\n",
    "                    count = count+1\n",
    "            list.append((count/len(tex_check))*100)  \n",
    "        l = []\n",
    "        count1 = 0 \n",
    "        maxi = max(list)\n",
    "        for val in list:\n",
    "            if(val==maxi):\n",
    "                l.append(count1)\n",
    "            count1 = count1+1 \n",
    "        l2=[]\n",
    "        for j in l:\n",
    "            l2.append(data['Term'][j])\n",
    "        t_list.append(l2)    \n",
    "    trial['possible'] = t_list    \n",
    "    final_list = []\n",
    "    fill = []\n",
    "    for i in range(0,len(db)):\n",
    "        metric = db['possible'][i]\n",
    "        final = []\n",
    "        for j in range(0,len(trial)):\n",
    "            count = 0\n",
    "            for k in trial['possible'][j]:\n",
    "                if(k in metric):\n",
    "                    count = count+1\n",
    "            final.append((count/len(trial['possible'][j]))*100)\n",
    "            l3 =[]\n",
    "            for k in range(0,len(final)):\n",
    "                if(final[k]>0):\n",
    "                    l3.append(trial['trial'][k])\n",
    "        if(len(l3)==len(final)):\n",
    "            final_list.append(fill)\n",
    "        else:    \n",
    "            final_list.append(l3) \n",
    "    return(final_list)       \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric_name</th>\n",
       "      <th>possible</th>\n",
       "      <th>matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accounts Payable</td>\n",
       "      <td>[Accounts Payable (AP) , Accounts Payable Subs...</td>\n",
       "      <td>[FCF &amp; Balance Sheet Model, D&amp;A, FAS 123, Pari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accounts Receivable</td>\n",
       "      <td>[Accounts Receivable (AR) , Accounts Receivabl...</td>\n",
       "      <td>[FCF &amp; Balance Sheet Model, D&amp;A, FAS 123, Pari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adjusted EBITDA</td>\n",
       "      <td>[Adjusted Balance Method , Adjusted Basis , Ad...</td>\n",
       "      <td>[Free Cash Flow Model, D&amp;A, FAS 123, Gross Fre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adjusted EBITDA Before Rent Expense</td>\n",
       "      <td>[Accrued Expense , Adjusted Earnings , Adjuste...</td>\n",
       "      <td>[FCF &amp; Balance Sheet Model, D&amp;A, FAS 123, Less...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adjusted EBITDA Margin (% of Total Revenues)</td>\n",
       "      <td>[EBITDA Margin ]</td>\n",
       "      <td>[D&amp;A, Parisian Macau, Venetian Macau, Other, T...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    metric_name  \\\n",
       "0                              Accounts Payable   \n",
       "1                           Accounts Receivable   \n",
       "2                               Adjusted EBITDA   \n",
       "3           Adjusted EBITDA Before Rent Expense   \n",
       "4  Adjusted EBITDA Margin (% of Total Revenues)   \n",
       "\n",
       "                                            possible  \\\n",
       "0  [Accounts Payable (AP) , Accounts Payable Subs...   \n",
       "1  [Accounts Receivable (AR) , Accounts Receivabl...   \n",
       "2  [Adjusted Balance Method , Adjusted Basis , Ad...   \n",
       "3  [Accrued Expense , Adjusted Earnings , Adjuste...   \n",
       "4                                   [EBITDA Margin ]   \n",
       "\n",
       "                                             matches  \n",
       "0  [FCF & Balance Sheet Model, D&A, FAS 123, Pari...  \n",
       "1  [FCF & Balance Sheet Model, D&A, FAS 123, Pari...  \n",
       "2  [Free Cash Flow Model, D&A, FAS 123, Gross Fre...  \n",
       "3  [FCF & Balance Sheet Model, D&A, FAS 123, Less...  \n",
       "4  [D&A, Parisian Macau, Venetian Macau, Other, T...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = pd.read_csv(\"lvs_actual_metric.csv\", error_bad_lines=False)\n",
    "trial = pd.read_csv(\"test1.csv\", error_bad_lines=False)\n",
    "final_matches = return_match_list(db,trial)\n",
    "db['matches'] = final_matches\n",
    "db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
